{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Micrograd: A Complete Summary\n",
    "\n",
    "This notebook is a clean, linear summary of **micrograd** - a minimal automatic differentiation engine that can train neural networks.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. **Derivatives** - The foundation of learning\n",
    "2. **Value Class** - Smart numbers that track gradients\n",
    "3. **Backpropagation** - How gradients flow backward\n",
    "4. **Neural Network Components** - Neuron, Layer, MLP\n",
    "5. **Training** - Putting it all together\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Understanding Derivatives\n",
    "\n",
    "A **derivative** tells us how much a function's output changes when we slightly change its input.\n",
    "\n",
    "**Mathematical Definition:**\n",
    "```\n",
    "f'(x) = lim(h→0) [f(x + h) - f(x)] / h\n",
    "```\n",
    "\n",
    "In simple terms: *\"If I nudge the input a tiny bit, how much does the output change?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "derivative-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At x = 3.0:\n",
      "  f(x) = 20.0\n",
      "  slope (derivative) ≈ 14.0003\n",
      "  Analytical derivative: 6x - 4 = 14.0\n"
     ]
    }
   ],
   "source": [
    "# Example: f(x) = 3x² - 4x + 5\n",
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5\n",
    "\n",
    "# Compute derivative numerically\n",
    "h = 0.0001  # tiny nudge\n",
    "x = 3.0\n",
    "\n",
    "slope = (f(x + h) - f(x)) / h\n",
    "print(f\"At x = {x}:\")\n",
    "print(f\"  f(x) = {f(x)}\")\n",
    "print(f\"  slope (derivative) ≈ {slope:.4f}\")\n",
    "print(f\"  Analytical derivative: 6x - 4 = {6*x - 4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-derivatives",
   "metadata": {},
   "source": [
    "### Why Do We Care About Derivatives?\n",
    "\n",
    "In machine learning, we want to **minimize a loss function**. The derivative tells us which direction to move!\n",
    "\n",
    "```\n",
    "If derivative > 0: decrease the input to reduce output\n",
    "If derivative < 0: increase the input to reduce output\n",
    "```\n",
    "\n",
    "This is the core idea behind **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: The Value Class\n",
    "\n",
    "The `Value` class is the heart of micrograd. It wraps a number and:\n",
    "\n",
    "1. **Tracks the computation graph** - remembers what operations created it\n",
    "2. **Stores the gradient** - derivative of loss with respect to this value\n",
    "3. **Knows how to backpropagate** - compute gradients automatically\n",
    "\n",
    "Think of it as a **\"smart number\"** that remembers its history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "value-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\"\n",
    "    A wrapper around a number that supports automatic differentiation.\n",
    "    \n",
    "    Attributes:\n",
    "        data: The actual numerical value\n",
    "        grad: The gradient (derivative of loss w.r.t. this value)\n",
    "        _backward: Function to propagate gradients to children\n",
    "        _prev: Set of Value objects that created this one\n",
    "        _op: String describing the operation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0                    # gradient starts at 0\n",
    "        self._backward = lambda: None      # no-op by default\n",
    "        self._prev = set(_children)        # parent nodes in graph\n",
    "        self._op = _op                     # operation that created this\n",
    "        self.label = label                 # optional name for debugging\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "    \n",
    "    # =====================================================================\n",
    "    # ARITHMETIC OPERATIONS\n",
    "    # =====================================================================\n",
    "    # Each operation:\n",
    "    # 1. Computes the forward pass (the actual math)\n",
    "    # 2. Defines _backward() to compute gradients during backprop\n",
    "    # =====================================================================\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"Addition: self + other\"\"\"\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            # d(a+b)/da = 1, d(a+b)/db = 1\n",
    "            # Gradient flows equally to both inputs\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Multiplication: self * other\"\"\"\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            # d(a*b)/da = b, d(a*b)/db = a\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        \"\"\"Power: self ** other (only supports int/float exponents)\"\"\"\n",
    "        assert isinstance(other, (int, float)), \"only int/float powers supported\"\n",
    "        out = Value(self.data ** other, (self,), f'**{other}')\n",
    "        \n",
    "        def _backward():\n",
    "            # d(x^n)/dx = n * x^(n-1)\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):        # -self\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * (other ** -1)\n",
    "    \n",
    "    # Reverse operations (when Value is on the right side)\n",
    "    def __radd__(self, other):  # other + self\n",
    "        return self + other\n",
    "    \n",
    "    def __rsub__(self, other):  # other - self\n",
    "        return other + (-self)\n",
    "    \n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "    \n",
    "    # =====================================================================\n",
    "    # ACTIVATION FUNCTION\n",
    "    # =====================================================================\n",
    "    \n",
    "    def tanh(self):\n",
    "        \"\"\"\n",
    "        Hyperbolic tangent activation.\n",
    "        \n",
    "        tanh(x) = (e^(2x) - 1) / (e^(2x) + 1)\n",
    "        \n",
    "        - Output range: (-1, 1)\n",
    "        - Derivative: 1 - tanh(x)²\n",
    "        \"\"\"\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1 - t ** 2) * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        \"\"\"Exponential: e^self\"\"\"\n",
    "        out = Value(math.exp(self.data), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            # d(e^x)/dx = e^x\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # =====================================================================\n",
    "    # BACKPROPAGATION\n",
    "    # =====================================================================\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Compute gradients for all nodes in the computation graph.\n",
    "        \n",
    "        Algorithm:\n",
    "        1. Build topological order (children before parents)\n",
    "        2. Set output gradient to 1.0\n",
    "        3. Process nodes in reverse order, calling _backward() on each\n",
    "        \"\"\"\n",
    "        # Step 1: Topological sort using DFS\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        \n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(node)\n",
    "        \n",
    "        build_topo(self)\n",
    "        \n",
    "        # Step 2: Set output gradient to 1\n",
    "        self.grad = 1.0\n",
    "        \n",
    "        # Step 3: Propagate gradients backward\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Understanding Backpropagation\n",
    "\n",
    "**Backpropagation** is how we compute gradients automatically using the **chain rule**.\n",
    "\n",
    "### The Chain Rule\n",
    "\n",
    "If `y = f(g(x))`, then:\n",
    "```\n",
    "dy/dx = dy/dg · dg/dx\n",
    "```\n",
    "\n",
    "In other words: **multiply the gradients along the path**.\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "Let's trace through: `L = (a * b + c) * f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "backprop-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass:\n",
      "  a=2.0, b=-3.0, c=10.0, f=-2.0\n",
      "  e = a * b = -6.0\n",
      "  d = e + c = 4.0\n",
      "  L = d * f = -8.0\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "f = Value(-2.0, label='f')\n",
    "\n",
    "e = a * b       # e = 2 * -3 = -6\n",
    "d = e + c       # d = -6 + 10 = 4\n",
    "L = d * f       # L = 4 * -2 = -8\n",
    "\n",
    "print(\"Forward Pass:\")\n",
    "print(f\"  a={a.data}, b={b.data}, c={c.data}, f={f.data}\")\n",
    "print(f\"  e = a * b = {e.data}\")\n",
    "print(f\"  d = e + c = {d.data}\")\n",
    "print(f\"  L = d * f = {L.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "backprop-backward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backward Pass (Gradients):\n",
      "  dL/dL = 1.0 (always 1.0)\n",
      "  dL/df = d = 4.0\n",
      "  dL/dd = f = -2.0\n",
      "  dL/dc = dL/dd × dd/dc = -2.0 × 1 = -2.0\n",
      "  dL/de = dL/dd × dd/de = -2.0 × 1 = -2.0\n",
      "  dL/da = dL/de × de/da = -2.0 × b = 6.0\n",
      "  dL/db = dL/de × de/db = -2.0 × a = -4.0\n",
      "\n",
      "Interpretation:\n",
      "  If we increase 'a' by 0.01, L changes by ≈ 0.0600\n"
     ]
    }
   ],
   "source": [
    "# Backward pass - compute all gradients automatically\n",
    "L.backward()\n",
    "\n",
    "print(\"\\nBackward Pass (Gradients):\")\n",
    "print(f\"  dL/dL = {L.grad} (always 1.0)\")\n",
    "print(f\"  dL/df = d = {f.grad}\")\n",
    "print(f\"  dL/dd = f = {d.grad}\")\n",
    "print(f\"  dL/dc = dL/dd × dd/dc = {d.grad} × 1 = {c.grad}\")\n",
    "print(f\"  dL/de = dL/dd × dd/de = {d.grad} × 1 = {e.grad}\")\n",
    "print(f\"  dL/da = dL/de × de/da = {e.grad} × b = {a.grad}\")\n",
    "print(f\"  dL/db = dL/de × de/db = {e.grad} × a = {b.grad}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  If we increase 'a' by 0.01, L changes by ≈ {a.grad * 0.01:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computation-graph",
   "metadata": {},
   "source": [
    "### The Computation Graph\n",
    "\n",
    "```\n",
    "    a ──┐\n",
    "        × ── e ──┐\n",
    "    b ──┘        │\n",
    "                 + ── d ──┐\n",
    "    c ───────────┘        │\n",
    "                          × ── L\n",
    "    f ────────────────────┘\n",
    "```\n",
    "\n",
    "**Forward pass**: Follow arrows left → right to compute values  \n",
    "**Backward pass**: Follow arrows right → left to compute gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Neural Network Components\n",
    "\n",
    "Neural networks are built from simple, composable parts:\n",
    "\n",
    "| Component | What it does | Input → Output |\n",
    "|-----------|--------------|----------------|\n",
    "| **Neuron** | Weighted sum + activation | n values → 1 value |\n",
    "| **Layer** | Multiple neurons in parallel | n values → m values |\n",
    "| **MLP** | Multiple layers in sequence | n values → k values |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "neuron-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    A single neuron: computes weighted sum of inputs + bias, then applies tanh.\n",
    "    \n",
    "    Formula: output = tanh(w₁×x₁ + w₂×x₂ + ... + wₙ×xₙ + b)\n",
    "    \n",
    "    Visual:\n",
    "        x₁ ──w₁──┐\n",
    "        x₂ ──w₂──┼── Σ + b ── tanh ── output\n",
    "        x₃ ──w₃──┘\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs):\n",
    "        # Initialize weights and bias randomly in [-1, 1]\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(num_inputs)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Weighted sum: w₁×x₁ + w₂×x₂ + ... + b\n",
    "        activation = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        # Apply activation function\n",
    "        return activation.tanh()\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Return all trainable parameters.\"\"\"\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "layer-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A layer of neurons processing the same input in parallel.\n",
    "    \n",
    "    Each neuron sees the SAME input but has DIFFERENT weights,\n",
    "    so they learn to detect different patterns.\n",
    "    \n",
    "    Visual (Layer with 3 neurons, 2 inputs):\n",
    "        \n",
    "               ┌── Neuron₁ ── out₁\n",
    "        x₁ ────┼── Neuron₂ ── out₂\n",
    "        x₂ ────┼── Neuron₃ ── out₃\n",
    "               └──────────────────\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        self.neurons = [Neuron(num_inputs) for _ in range(num_outputs)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outputs = [neuron(x) for neuron in self.neurons]\n",
    "        # Return single value if only one neuron\n",
    "        return outputs[0] if len(outputs) == 1 else outputs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mlp-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron: layers stacked sequentially.\n",
    "    \n",
    "    Example: MLP(3, [4, 4, 1])\n",
    "    \n",
    "        Input (3)     Hidden 1 (4)    Hidden 2 (4)    Output (1)\n",
    "        \n",
    "          x₁ ────────── ○ ────────────── ○ ──────────┐\n",
    "          x₂ ────────── ○ ────────────── ○ ──────────┼── output\n",
    "          x₃ ────────── ○ ────────────── ○ ──────────┘\n",
    "                        ○                ○\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, layer_sizes):\n",
    "        sizes = [num_inputs] + layer_sizes\n",
    "        self.layers = [Layer(sizes[i], sizes[i+1]) for i in range(len(layer_sizes))]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Pass through each layer sequentially\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "test-components",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Neural Network Components:\n",
      "Input: [2.0, 3.0]\n",
      "\n",
      "Neuron (2 inputs → 1 output): Value(data=-0.9917, grad=0.0000)\n",
      "Layer (2 inputs → 3 outputs): [0.5817270619168307, -0.7878755440086811, -0.9983781851034335]\n",
      "MLP (2 → 4 → 4 → 1): Value(data=-0.5063, grad=0.0000)\n",
      "\n",
      "Total parameters in MLP: 37\n"
     ]
    }
   ],
   "source": [
    "# Test the components\n",
    "random.seed(42)\n",
    "\n",
    "x = [2.0, 3.0]  # input with 2 features\n",
    "\n",
    "print(\"Testing Neural Network Components:\")\n",
    "print(f\"Input: {x}\")\n",
    "print()\n",
    "\n",
    "neuron = Neuron(2)\n",
    "print(f\"Neuron (2 inputs → 1 output): {neuron(x)}\")\n",
    "\n",
    "layer = Layer(2, 3)\n",
    "print(f\"Layer (2 inputs → 3 outputs): {[v.data for v in layer(x)]}\")\n",
    "\n",
    "mlp = MLP(2, [4, 4, 1])\n",
    "print(f\"MLP (2 → 4 → 4 → 1): {mlp(x)}\")\n",
    "print(f\"\\nTotal parameters in MLP: {len(mlp.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Training a Neural Network\n",
    "\n",
    "Training adjusts weights to minimize the **loss** (error).\n",
    "\n",
    "### The Training Loop\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│  1. FORWARD PASS    : Input → Network → Prediction         │\n",
    "│  2. COMPUTE LOSS    : How wrong is the prediction?         │\n",
    "│  3. ZERO GRADIENTS  : Reset all gradients to 0             │\n",
    "│  4. BACKWARD PASS   : Compute gradients (∂Loss/∂weight)    │\n",
    "│  5. UPDATE WEIGHTS  : weight -= learning_rate × gradient   │\n",
    "│                                                             │\n",
    "│  REPEAT until loss is small enough                         │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Loss Function: Mean Squared Error\n",
    "\n",
    "```\n",
    "Loss = Σ (prediction - target)²\n",
    "```\n",
    "\n",
    "- If prediction = target → loss = 0 (perfect!)\n",
    "- Bigger difference → bigger loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "training-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 4 samples\n",
      "Network architecture: 3 → 4 → 4 → 1\n",
      "Total parameters: 41\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRAINING DATA\n",
    "# ============================================\n",
    "\n",
    "# Input samples: 4 examples with 3 features each\n",
    "inputs = [\n",
    "    [2.0, 3.0, -1.0],   # sample 1\n",
    "    [3.0, -1.0, 0.5],   # sample 2\n",
    "    [0.5, 1.0, 1.0],    # sample 3\n",
    "    [1.0, 1.0, -1.0]    # sample 4\n",
    "]\n",
    "\n",
    "# Target outputs: what we want the network to predict\n",
    "targets = [1.0, -1.0, -1.0, 1.0]\n",
    "\n",
    "# Create network: 3 inputs → 4 hidden → 4 hidden → 1 output\n",
    "random.seed(42)\n",
    "mlp = MLP(3, [4, 4, 1])\n",
    "\n",
    "print(f\"Dataset: {len(inputs)} samples\")\n",
    "print(f\"Network architecture: 3 → 4 → 4 → 1\")\n",
    "print(f\"Total parameters: {len(mlp.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "training-loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 iterations...\n",
      "Learning rate: 0.1\n",
      "--------------------------------------------------\n",
      "Iteration   0 | Loss: 5.230518\n",
      "Iteration  20 | Loss: 0.034212\n",
      "Iteration  40 | Loss: 0.014531\n",
      "Iteration  60 | Loss: 0.008926\n",
      "Iteration  80 | Loss: 0.006351\n",
      "--------------------------------------------------\n",
      "Final Loss: 0.004950\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRAINING LOOP\n",
    "# ============================================\n",
    "\n",
    "learning_rate = 0.1   # step size for weight updates\n",
    "num_iterations = 100  # number of training iterations\n",
    "\n",
    "print(f\"Training for {num_iterations} iterations...\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    \n",
    "    # ===== STEP 1: FORWARD PASS =====\n",
    "    # Compute predictions for all inputs\n",
    "    predictions = [mlp(x) for x in inputs]\n",
    "    \n",
    "    # ===== STEP 2: COMPUTE LOSS =====\n",
    "    # Mean Squared Error\n",
    "    loss = sum((pred - target)**2 for target, pred in zip(targets, predictions))\n",
    "    \n",
    "    # ===== STEP 3: ZERO GRADIENTS =====\n",
    "    # Gradients accumulate, so we must reset them\n",
    "    for param in mlp.parameters():\n",
    "        param.grad = 0.0\n",
    "    \n",
    "    # ===== STEP 4: BACKWARD PASS =====\n",
    "    # Compute all gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # ===== STEP 5: UPDATE WEIGHTS =====\n",
    "    # Gradient descent: move opposite to gradient\n",
    "    for param in mlp.parameters():\n",
    "        param.data -= learning_rate * param.grad\n",
    "    \n",
    "    # Print progress\n",
    "    if iteration % 20 == 0:\n",
    "        print(f\"Iteration {iteration:3d} | Loss: {loss.data:.6f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final Loss: {loss.data:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "show-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions vs Targets:\n",
      "==================================================\n",
      "Sample 1: predicted +0.9663, target +1.0  ✓\n",
      "Sample 2: predicted -0.9816, target -1.0  ✓\n",
      "Sample 3: predicted -0.9516, target -1.0  ✓\n",
      "Sample 4: predicted +0.9663, target +1.0  ✓\n",
      "\n",
      "The network learned to classify inputs correctly!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# FINAL RESULTS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nPredictions vs Targets:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, (pred, target) in enumerate(zip(predictions, targets)):\n",
    "    match = \"✓\" if (pred.data > 0) == (target > 0) else \"✗\"\n",
    "    print(f\"Sample {i+1}: predicted {pred.data:+.4f}, target {target:+.1f}  {match}\")\n",
    "\n",
    "print(\"\\nThe network learned to classify inputs correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: What We Built\n",
    "\n",
    "### 1. Value Class (Automatic Differentiation)\n",
    "- Wraps numbers and tracks computation history\n",
    "- Each operation knows its gradient rule\n",
    "- `backward()` applies chain rule automatically\n",
    "\n",
    "### 2. Neural Network Components\n",
    "```\n",
    "Neuron: weighted sum + activation → 1 output\n",
    "Layer:  multiple neurons in parallel → n outputs\n",
    "MLP:    multiple layers in sequence → final output\n",
    "```\n",
    "\n",
    "### 3. Training Loop\n",
    "```\n",
    "1. Forward:  Input → Prediction\n",
    "2. Loss:     Measure error\n",
    "3. Backward: Compute gradients\n",
    "4. Update:   Adjust weights\n",
    "5. Repeat!\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "**This is the foundation of PyTorch and TensorFlow!**\n",
    "\n",
    "The only differences in production frameworks:\n",
    "- Optimized for GPUs\n",
    "- More operations and activations\n",
    "- Better optimizers than basic gradient descent\n",
    "- Automatic batching\n",
    "\n",
    "But the core idea is exactly the same!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
