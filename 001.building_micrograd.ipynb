{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67959236",
   "metadata": {},
   "source": [
    "# Building Micrograd\n",
    "In this notebook, we will be learning gradient descent and how to implement it from scratch.\n",
    "\n",
    "Video Reference: [NEURAL NETWORKS: Zero to Hero - Building Micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb306e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ef044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating simple polynomial function\n",
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "f(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now just to visualize the function\n",
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will work on derivatives\n",
    "# Derivative: Definition of derivative is the slope of the tangent line to the curve at a point... Mathematically, it is defined as the limit of the average rate of change of the function as the interval approaches zero. i.e. f'(x) = lim(h->0) [f(x+h) - f(x)] / h\n",
    "h = 0.000001\n",
    "x = 3.0\n",
    "print(f'For x = {x}, f(x) = {f(x)}')\n",
    "print(f'For x = {x + h}, f(x + h) = {f(x + h)}')\n",
    "\n",
    "# now for the derivative or slope\n",
    "slope = (f(x + h) - f(x)) / h\n",
    "print(f'The slope at x = {x} is approximately {slope}')\n",
    "\n",
    "# Now at some point the slope will be equal to 0\n",
    "x = 2/3\n",
    "slope = (f(x + h) - f(x)) / h\n",
    "print(f'The slope at x = {x} is approximately {slope}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will try some different equation with many variable and try to get the derivative of that\n",
    "# let's get more complex\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "d = a*b + c\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf00746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start calculating the derivatives\n",
    "h = 0.0001\n",
    "\n",
    "# imputs\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "# now we will derive with respect to a, b and c\n",
    "d1 = a*b + c\n",
    "# a += h\n",
    "# b += h\n",
    "c += h\n",
    "d2 = a*b + c\n",
    "slope = (d2 - d1) / h\n",
    "print(f'D1: {d1}, D2: {d2}, Slope: {slope}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15917167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "  def __init__(self, data, _children=(), _op='', label = ''):\n",
    "    \"\"\"\n",
    "    Creating Initial Value\n",
    "    Args:\n",
    "      data: some numerical value\n",
    "      grad: gradient of the value (derivative)\n",
    "      _backward: function to propagate the gradient backward\n",
    "      _children: a tuple of previous operations\n",
    "      _op: symbol representing the operation (+, *, etc.)\n",
    "      label: a string label for the value (name of the variable for visualization)\n",
    "    \"\"\"\n",
    "    self.data = data\n",
    "    self.grad = 0.0\n",
    "    self._backward = lambda: None\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  def __add__(self, other):\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += 1.0 * out.grad\n",
    "      other.grad += 1.0 * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  def tanh(self):\n",
    "    x = self.data\n",
    "    t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "\n",
    "    out = Value(t, (self,), 'tanh')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += (1 - t ** 2) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  def backward(self):\n",
    "    # Build the topological order\n",
    "    topo = []\n",
    "    visited = set()\n",
    "\n",
    "    def build_topo(node):\n",
    "      if node not in visited:\n",
    "        visited.add(node)\n",
    "        for child in node._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(node)\n",
    "\n",
    "    build_topo(self)\n",
    "\n",
    "    # Reverse the topological order\n",
    "    self.grad = 1.0\n",
    "    for node in reversed(topo):\n",
    "      node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ec77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is some copy pasted code that will help visualize the operation chain and it will make sense why are we storing the previous values and operations.\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot\n",
    "\n",
    "# HOW TO USE\n",
    "# You can use the draw_dot function to visualize the computation graph of a Value object.\n",
    "# Simply call draw_dot(d) where d is the Value object you want to visualize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6441e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a * b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label = 'L'\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol():\n",
    "  \n",
    "  h = 0.0001\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10.0, label='c')\n",
    "  e = a * b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d * f; L.label = 'L'\n",
    "  L1 = L.data\n",
    "  \n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10.0, label='c')\n",
    "  e = a * b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d * f; L.label = 'L'\n",
    "  L2 = L.data + h\n",
    "\n",
    "  # printing the derivatives\n",
    "  print(\"L1:\", L1)\n",
    "  print(\"L2:\", L2)\n",
    "  print(\"Slope:\", (L2 - L1) / h)\n",
    "\n",
    "lol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de80a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual back propagation\n",
    "\n",
    "# Let's calculate gradients\n",
    "# dL/dL = 1.0\n",
    "L.grad = 1.0\n",
    "\n",
    "# dL/dd = f, dL/df = d\n",
    "d.grad = L.grad * f.data\n",
    "f.grad = L.grad * d.data\n",
    "\n",
    "# dL/de = dL/dd * dd/de = f * 1 = f\n",
    "e.grad = d.grad * 1.0\n",
    "\n",
    "# dL/dc = dL/dd * dd/dc = f * 1 = f\n",
    "c.grad = d.grad * 1.0\n",
    "\n",
    "# dL/db = dL/de * de/db = f * a\n",
    "b.grad = e.grad * a.data\n",
    "\n",
    "# dL/da = dL/de * de/da = f * b\n",
    "a.grad = e.grad * b.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c917c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual forward pass is by nudging the inputs by a small amount of gradient we reduce the loss\n",
    "\n",
    "a.data += 0.01 * a.grad\n",
    "b.data += 0.01 * b.grad\n",
    "c.data += 0.01 * c.grad\n",
    "f.data += 0.01 * f.grad\n",
    "\n",
    "# lets calculate the loss again\n",
    "e = a * b\n",
    "d = e + c\n",
    "L = d * f\n",
    "\n",
    "print(L.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178db71a",
   "metadata": {},
   "source": [
    "---\n",
    "Let do one more manual back propogation with a bit more complex equation which will look like a neural network layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following is an equation for a simple neuron with two inputs\n",
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "# apply the tanh activation function\n",
    "o = n.tanh(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual backpropagation\n",
    "\n",
    "# do/do = 1.0\n",
    "o.grad = 1.0\n",
    "\n",
    "# eq: o = tanh(n)\n",
    "# do/dn = d(tanh(n))/dn = 1 - tanh(n)^2 = 1 - o.data^2\n",
    "n.grad = o.grad * (1 - o.data ** 2) # n.grad =  0.5\n",
    "\n",
    "# eq: n = x1w1x2w2 + b\n",
    "# dn/d(x1w1x2w2) = 1\n",
    "# do/d(x1w1x2w2) = do/dn * dn/d(x1w1x2w2) = do/dn\n",
    "x1w1x2w2.grad = n.grad * 1.0 # x1w1x2w2.grad = 0.5\n",
    "# dn/db = 1\n",
    "# do/db = do/dn * dn/db = do/dn\n",
    "b.grad = n.grad * 1.0 # b.grad = 0.5\n",
    "\n",
    "# eq: x1w1x2w2 = x1w1 + x2w2\n",
    "# dx1w1x2w2/dx1w1 = 1\n",
    "# do/dx1w1 = do/d(x1w1x2w2) * d(x1w1x2w2)/dx1w1 = do/dn\n",
    "x1w1.grad = x1w1x2w2.grad * 1.0 # x1w1.grad = 0.5\n",
    "# dx1w1x2w2/dx2w2 = 1\n",
    "# do/dx2w2 = do/d(x1w1x2w2) * d(x1w1x2w2)/dx2w2 = do/dn\n",
    "x2w2.grad = x1w1x2w2.grad * 1.0 # x2w2.grad = 0.5\n",
    "\n",
    "# eq: x1w1 = x1*w1\n",
    "# dx1w1/dx1 = w1\n",
    "# do/dx1 = do/d(x1w1) * dx1w1/dx1 = do/dn * w1\n",
    "x1.grad = x1w1.grad * w1.data # x1.grad = -1.5\n",
    "# dx1w1/dw1 = x1\n",
    "# do/dw1 = do/d(x1w1) * dx1w1/dw1 = do/dn * x1\n",
    "w1.grad = x1w1.grad * x1.data # w1.grad = 1.0\n",
    "\n",
    "# eq: x2w2 = x2*w2\n",
    "# dx2w2/dx2 = w2\n",
    "# do/dx2 = do/d(x2w2) * dx2w2/dx2 = do/dn * w2\n",
    "x2.grad = x2w2.grad * w2.data # x2.grad = 0.5\n",
    "# dx2w2/dw2 = x2\n",
    "# do/dw2 = do/d(x2w2) * dx2w2/dw2 = do/dn * x2\n",
    "w2.grad = x2w2.grad * x2.data # w2.grad = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do the backprogation using the _backward function\n",
    "o.grad = 1.0\n",
    "o._backward()\n",
    "n._backward()\n",
    "b._backward()\n",
    "x1w1x2w2._backward()\n",
    "x1w1._backward()\n",
    "x2w2._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b76bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now since we don't want to do this manually, let's do a topological sort of the graph to determine the order of operations.\n",
    "\n",
    "topo = []          # stores nodes in topological order\n",
    "visited = set()    # tracks visited nodes\n",
    "\n",
    "def build_topo(v):\n",
    "  if v not in visited:\n",
    "    visited.add(v)         # mark node as visited\n",
    "    for child in v._prev:  # recursively visit dependencies\n",
    "      build_topo(child)\n",
    "    topo.append(v)         # add node AFTER its children\n",
    "\n",
    "build_topo(o)              # start from output node 'o'\n",
    "topo                       # now contains nodes in order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4412558",
   "metadata": {},
   "source": [
    "### Understanding `build_topo`\n",
    "\n",
    "The goal of backpropagation is to traverse the computation graph in reverse, so that\n",
    "gradients flow from the output node back to the inputs.\n",
    "\n",
    "To do this, we use **topological sorting**:\n",
    "- Visit each node’s dependencies (children) first.\n",
    "- Add the node itself after all its children have been added. (that means if x = a + b, we add a and b in `topo` after that we add x)\n",
    "- This ensures inputs come before outputs.\n",
    "\n",
    "The result is a list (`topo`) of nodes in the correct order for backprop.\n",
    "When we reverse this list, we can propagate gradients correctly.\n",
    "\n",
    "Example for our neuron:\n",
    "```\n",
    "# Inputs\n",
    "x1w1 = x1 * w1\n",
    "x2w2 = x2 * w2\n",
    "x1w1x2w2 = x1w1 + x2w2\n",
    "n = x1w1x2w2 + b\n",
    "o = tanh(n)\n",
    "\n",
    "# graph\n",
    "x1 ─┐\n",
    "    * → x1w1 ─┐\n",
    "w1 ─┘          \\\n",
    "                + → x1w1x2w2 ─┐\n",
    "x2 ─┐           /              \\\n",
    "    * → x2w2 ──┘                + → n → tanh → o\n",
    "w2 ─┘                           /\n",
    "                               b\n",
    "\n",
    "\n",
    "# Topological order\n",
    "topo = [x1, w1, x1w1, x2, w2, x2w2, x1w1x2w2, b, n, o]\n",
    "```\n",
    "\n",
    "`topo` will store them in this order, children before parents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739768ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to do the backpropagation in order we will first run the build_topo cell. Then do the backward pass on the reversed topo list.\n",
    "\n",
    "o.grad = 1.0 # this is important to set so all our values have a gradient\n",
    "print(f\"Topological order: {[name.label for name in topo]}\")\n",
    "for node in reversed(topo):\n",
    "    node._backward()\n",
    "\n",
    "draw_dot(o)\n",
    "\n",
    "# now since we don't want to do this again and again of calculating the topological order, and then doing reverse and then doing _backward on it. We will create a function in Value class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d952bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we can do directly a backward pass like this\n",
    "o.backward()\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# few more examples\n",
    "a = Value(3.0, label='a')\n",
    "b = a + a\n",
    "b.backward()\n",
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81030bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(-2.0, label='a')\n",
    "b = Value(3.0, label='b')\n",
    "d = a * b; d.label = 'd'\n",
    "e = a + b; e.label = 'e'\n",
    "f = d * e; f.label = 'f'\n",
    "\n",
    "f.backward()\n",
    "draw_dot(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-network-learning-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
