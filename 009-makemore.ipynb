{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4968d63d-419f-4909-acb7-c145a591141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('009-makemore_names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b87be54-6cb1-4741-a2dd-e78c775f5a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26152a3f-a074-4392-80ed-02e0a1d65662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1524368-2a71-42b9-b5d4-c4df4dbf5e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f52d74ba-a09f-40e2-b397-a2c62ef25f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303b1ee-14c4-4340-add9-2de89303dadb",
   "metadata": {},
   "source": [
    "# Bigram\n",
    "first we will try to just predict the next charcter from the list of words based on the previous word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23185bae-a5d2-4a88-84ef-454548150ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a <E>\n",
      "<S> o\n",
      "o l\n",
      "l i\n",
      "i v\n",
      "v i\n",
      "i a\n",
      "a <E>\n",
      "<S> a\n",
      "a v\n",
      "v a\n",
      "a <E>\n"
     ]
    }
   ],
   "source": [
    "# lets just take the first word and see how to pair each character with the next one\n",
    "for w in words[:3]: # lets just do it on first word\n",
    "    # lets wrap the word with special user defined term that will define the start and the end before and after the word\n",
    "    new_word = ['<S>'] + list(w) + ['<E>'] # w = 'emma' and list(w) = ['e', 'm', 'm', 'a']\n",
    "    for char1, char2 in zip(new_word, new_word[1:]):\n",
    "        print(char1, char2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe57782b-1f37-4219-88ac-17261ddfb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will do is calculate how many times we get that pair i.e. calculate bigram frequency\n",
    "b = {}\n",
    "for w in words:\n",
    "    new_word = ['<S>'] + list(w) + ['<E>']\n",
    "    for char1, char2 in zip(new_word, new_word[1:]):\n",
    "        bigram = (char1, char2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76eb54-617f-4a49-92ea-ac876a160c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3de318ef-0dff-4128-9d1f-7ab1a3a6150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string to int {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "int to string {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# we will store the bigram frequency count in an array instead of a dict\n",
    "# so we will use the torch library to create arrays called tensor to store the bigram info\n",
    "\n",
    "import torch\n",
    "\n",
    "# now we will create 27 by 27 array since we have 26 alphabet and 1 special user defined character\n",
    "N = torch.zeros((27,27), dtype = torch.int32)\n",
    "\n",
    "# here the N array store on int number so we will map all alphabet + special character to an integer\n",
    "chars = sorted(list(set(''.join(words)))) # all character as list from words a to z\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # stoi is string to int, enumerate counts chars as integer in list based on index\n",
    "stoi['.'] = 0 # manually define special char mapping and instead of 2 special charcter we will have just one at the start and end as dot(.)\n",
    "# {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
    "itos = {i:s for s,i in stoi.items()} # int to string\n",
    "print('string to int', stoi)\n",
    "print('int to string', itos)\n",
    "\n",
    "# now in the 2D array will will increase the count of those cell which occur more frequency \n",
    "for w in words:\n",
    "    new_word = ['.'] + list(w) + ['.'] # wrapping word with special char\n",
    "    for char1, char2 in zip(new_word, new_word[1:]):\n",
    "        index_row = stoi[char1] # char1 in x coordinate or row\n",
    "        index_col = stoi[char2] # char2 in y coordinate or col\n",
    "        N[index_row, index_col] += 1 # increase the count of that cell in x,y coordinate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0fc50b-a70e-4a19-945b-6f3f3b7e4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the character pair frequency or Bigram \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# this visual is like a heat map of the frequency of the pair of character... the darker shade if it occurs more\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f789c11-8b70-4343-8258-1f3245bd5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets calculate the probablity for each cell\n",
    "probability = N.float()\n",
    "probability = probability / probability.sum(1, keepdim=True) # .sum on tensor we can mention to sum across the rows by saying 1 and keep dimension as true so probability.shape() = [27,1] for dividing with another tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "235f2b41-da89-4a2e-abaf-7cc5d2dcc541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "momasurailezitynn.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n"
     ]
    }
   ],
   "source": [
    "# here we have seed so all prediction is always the same with same randomness\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# lets predict 5 words based on probability of which character will come after previous\n",
    "for i in range(5):\n",
    "  out = []\n",
    "  index = 0 # starting from dot row or the first row\n",
    "  while True:\n",
    "    p = probability[index]\n",
    "    index = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() # based on probability predict next character\n",
    "    out.append(itos[index]) # list all predicted character\n",
    "\n",
    "    # if predicted character is dot in int term its 0 so we end loop. Because dot is the ending character\n",
    "    if index == 0:\n",
    "      break\n",
    "  print(''.join(out))\n",
    "\n",
    "# cexze.\n",
    "# momasurailezitynn.\n",
    "# konimittain.\n",
    "# llayn.\n",
    "# ka.\n",
    "\n",
    "# You will see words which doesn't look like name beacuse it is just predicting a next character it does not refer to all the previous character. Bigram is very stupid and simple by just predicting next charcted based on the previous one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
