{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982f6f1d",
   "metadata": {},
   "source": [
    "# Build Micrograd (Part 2)\n",
    "\n",
    "Video Resource: [YouTube - Micrograd Part 2](https://www.youtube.com/watch?v=VMj-3S1tku0)\n",
    "\n",
    "Now in the first half of the video we built the basic structure of Micrograd and also understood the backpropagation process manually and with topological sorting.\n",
    "\n",
    "In this second part, we will start with a clean slate so the code is not all messy and we just have the important parts for automatic differentiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eed110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "  def __init__(self, data, _children=(), _op='', label = ''):\n",
    "    \"\"\"\n",
    "    Creating Initial Value\n",
    "    Args:\n",
    "      data: some numerical value\n",
    "      grad: gradient of the value (derivative)\n",
    "      _backward: function to propagate the gradient backward\n",
    "      _children: a tuple of previous operations\n",
    "      _op: symbol representing the operation (+, *, etc.)\n",
    "      label: a string label for the value (name of the variable for visualization)\n",
    "    \"\"\"\n",
    "    self.data = data\n",
    "    self.grad = 0.0\n",
    "    self._backward = lambda: None\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += 1.0 * out.grad\n",
    "      other.grad += 1.0 * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def __pow__(self, other): # self ** other\n",
    "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def __neg__(self): # -self\n",
    "    return self * -1\n",
    "  \n",
    "  def __sub__(self, other): # self - other\n",
    "    return self + (-other)\n",
    "\n",
    "  # This is a fallback for __mul__, so if self * other fails we do other * self\n",
    "  def __rmul__(self, other): # other * self\n",
    "    return self * other\n",
    "\n",
    "  def __truediv__(self, other): # self / other\n",
    "    return self * other**-1\n",
    "\n",
    "  def tanh(self):\n",
    "    x = self.data\n",
    "    t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "\n",
    "    out = Value(t, (self,), 'tanh')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += (1 - t ** 2) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def exp(self):\n",
    "    x = self.data\n",
    "    out = Value(math.exp(x), (self,), 'exp')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += out.data * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  def backward(self):\n",
    "    # Build the topological order\n",
    "    topo = []\n",
    "    visited = set()\n",
    "\n",
    "    def build_topo(node):\n",
    "      if node not in visited:\n",
    "        visited.add(node)\n",
    "        for child in node._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(node)\n",
    "\n",
    "    build_topo(self)\n",
    "\n",
    "    # Reverse the topological order\n",
    "    self.grad = 1.0\n",
    "    for node in reversed(topo):\n",
    "      node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e9b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is some copy pasted code that will help visualize the operation chain and it will make sense why are we storing the previous values and operations.\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot\n",
    "\n",
    "# HOW TO USE\n",
    "# You can use the draw_dot function to visualize the computation graph of a Value object.\n",
    "# Simply call draw_dot(d) where d is the Value object you want to visualize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following is an equation for a simple neuron with two inputs\n",
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "# apply the tanh activation function\n",
    "o = n.tanh(); o.label = 'o'\n",
    "\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96977d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets edit the old equation so we use different way to do tanh\n",
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "# ------------\n",
    "e = (2*n).exp(); e.label = 'e'\n",
    "o = (e - 1) / (e + 1)\n",
    "# ------------\n",
    "o.label = 'o'\n",
    "\n",
    "o.backward()\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48979b",
   "metadata": {},
   "source": [
    "---\n",
    "## Using PyTorch\n",
    "\n",
    "Now we will do that same thing we did but a proper library (pytorch). This was some small learning that was based on PyTorch. PyTorch is a powerful library for building and training neural networks, it is used in production by many companies and researchers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26646a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in PyTorch we have torch.Tensor instead of Value which we used.\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b15967",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work on creating a neuron using micrograd\n",
    "\n",
    "class Neuron:\n",
    "\tdef __init__(self, nin):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the weights and bias for the neuron.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\tnin (int): The number of input connections to the neuron.\n",
    "\t\t\"\"\"\n",
    "\t\tself.w = [Value(random.uniform(-1,1)) for _ in range(nin)] # weights\n",
    "\t\tself.b = Value(random.uniform(-1,1)) # bias\n",
    "        \n",
    "\tdef __call__(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tThis function computes the output of the neuron for a given input. i.e. y = f(wx + b)\n",
    "\t\t\"\"\"\n",
    "\t\t# we will calculate the activation. i.e. Σ(wi*xi) + b. Here zip will create pairs of (wi, xi) in tuples and we will iterate on that to do wi*xi and sum them up starting from self.b instead of 0.0 for efficiency\n",
    "\t\tactivation = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "\n",
    "\t\t# now on top of that we will have an activation function. here it will be tanh\n",
    "\t\toutput = activation.tanh()\n",
    "\n",
    "\t\treturn output\n",
    "\t\n",
    "\n",
    "# now that we have created one single neuron using micrograd, we will create a layer\n",
    "# a layer is bunch of neurons working together\n",
    "class Layer:\n",
    "\tdef __init__(self, nin, nout):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the layer with a given number of input and output neurons.\n",
    "\t\tSo one neuron takes n number of inputs (nin) and produces one output.\n",
    "\t\tSo we will create n number of neurons (nout) in the layer to generate n number of outputs.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\tnin (int): The number of input connections for each neuron in the layer.\n",
    "\t\tnout (int): The number of neurons to create so we get n number of outputs.\n",
    "\t\t\"\"\"\n",
    "\t\tself.neurons = [Neuron(nin) for _ in range(nout)] # create nout neurons each with nin inputs\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tThis function computes the output of the layer for a given input by passing the input through each neuron in the layer. That means all inputs are given to each neuron to generate list of outputs.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\tx (list of float): The input values to the layer.\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\tlist of Value: The output values from each neuron in the layer.\n",
    "\t\t\"\"\"\n",
    "\t\tout = [n(x) for n in self.neurons] # pass the input x through each neuron\n",
    "\n",
    "\t\treturn out\n",
    "\t\n",
    "\n",
    "# Now when we have layer of Layers we call it a Multi-Layer Perceptron (MLP)\n",
    "# A MLP is just mutliple layers stacked together to form a neural network\n",
    "class MLP:\n",
    "\tdef __init__(self, nin, nouts):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the MLP with a given number of input connections and a list of output sizes for each layer.\n",
    "\t\tSo we will create multiple layers where each layer has its own number of neurons.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\tnin (int): The number of input connection for the first layer.\n",
    "\t\tnouts (list of int): A list where each element represents the number of neurons in that layer.\n",
    "\t\t\"\"\"\n",
    "\t\tsizes = [nin] + nouts # sizes will be [input_size, layer1_size, layer2_size, ...]\n",
    "\t\tself.layers = [Layer(sizes[i], sizes[i+1]) for i in range(len(nouts))] # create layers based on sizes\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tThis function computes the output of the MLP for a given input by passing the input through each layer in sequence.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\tx (list of float): The input values to the MLP. The length of x should match the input size of the first layer.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\tlist of Value: The output values from the final layer of the MLP.\n",
    "\t\t\"\"\"\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer(x) # pass the input through each layer sequentially\n",
    "\t\treturn x\n",
    "\n",
    "x = [2.0, 3.0] # input to the neuron\n",
    "neuron = Neuron(len(x)) # create a neuron with inputs as the length of x\n",
    "print(f\"Output of neuron: {neuron(x)}\")\n",
    "\n",
    "layer = Layer(len(x), 3) # create a layer with 3 neurons each managing the same inputs\n",
    "print(f\"Output of layer: {layer(x)}\")\n",
    "\n",
    "mlp = MLP(len(x), [4,4,1]) # create a MLP with 2 hidden layers of 4 neurons each and 1 output neuron\n",
    "print(f\"Output of MLP: {mlp(x)}\")\n",
    "\n",
    "\n",
    "# Here what we create is a simple neural network where we have a list of values as input and we run those inputs through a MLP. MLP class generates multiple layers using the layer class and each layer creates multiple neurons using the Neuron class. The inputs travel in neurons from layer to layer until we get the final output. For example: MLP(len(x), [4,4,1]) means if we have input as [2.0, 3.0] then first input layer will be [2.0, 3.0] -> first hidden layer with 4 neurons each getting the same input and each neuron will produce 1 output i.e. 4 outputs in total -> second hidden layer with 4 neurons each getting the 4 outputs from previous layer as input and each neuron will produce 1 output i.e. 4 outputs in total -> final output layer with 1 neuron getting the 4 outputs from previous layer as input and producing 1 final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce942jwzdj",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding the Code Flow for the above Implementation of Neuron, Layer, and MLP Classes as Forward Pass.\n",
    "\n",
    "### 1. Neuron Class\n",
    "\n",
    "A **Neuron** is the basic unit. It takes `nin` inputs and produces 1 output.\n",
    "\n",
    "**Initialization (`__init__`):**\n",
    "```\n",
    "Neuron(2) creates:\n",
    "├── self.w = [Value(random), Value(random)]  ← 2 weights (one per input)\n",
    "└── self.b = Value(random)                    ← 1 bias\n",
    "```\n",
    "\n",
    "**Forward Pass (`__call__`):**\n",
    "```\n",
    "Input: x = [2.0, 3.0]\n",
    "\n",
    "Step 1: zip(weights, inputs) → [(w0, 2.0), (w1, 3.0)]\n",
    "Step 2: Compute weighted sum starting from bias\n",
    "        → b + (w0 * 2.0) + (w1 * 3.0)\n",
    "Step 3: Apply tanh activation\n",
    "        → tanh(weighted_sum) → output ∈ [-1, 1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Layer Class\n",
    "\n",
    "A **Layer** is a collection of neurons. Each neuron receives the **same input** but has **different weights**.\n",
    "\n",
    "**Initialization (`__init__`):**\n",
    "```\n",
    "Layer(2, 3) creates:\n",
    "└── self.neurons = [Neuron(2), Neuron(2), Neuron(2)]  ← 3 neurons, each with 2 inputs\n",
    "```\n",
    "\n",
    "**Forward Pass (`__call__`):**\n",
    "```\n",
    "Input: x = [2.0, 3.0]\n",
    "\n",
    "All neurons receive the SAME input:\n",
    "├── Neuron 0: [2.0, 3.0] → w0·x + b0 → tanh → out0\n",
    "├── Neuron 1: [2.0, 3.0] → w1·x + b1 → tanh → out1\n",
    "└── Neuron 2: [2.0, 3.0] → w2·x + b2 → tanh → out2\n",
    "\n",
    "Output: [out0, out1, out2]  ← 3 values (one per neuron)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. MLP Class (Multi-Layer Perceptron)\n",
    "\n",
    "An **MLP** stacks multiple layers. The output of one layer becomes the input to the next.\n",
    "\n",
    "**Initialization (`__init__`):**\n",
    "```\n",
    "MLP(2, [4, 4, 1]) creates:\n",
    "\n",
    "sizes = [2, 4, 4, 1]  ← [input_size, layer1, layer2, layer3]\n",
    "\n",
    "self.layers:\n",
    "├── Layer(2, 4)  ← 4 neurons, each takes 2 inputs  → outputs 4 values\n",
    "├── Layer(4, 4)  ← 4 neurons, each takes 4 inputs  → outputs 4 values\n",
    "└── Layer(4, 1)  ← 1 neuron,  takes 4 inputs       → outputs 1 value\n",
    "```\n",
    "\n",
    "**Forward Pass (`__call__`):**\n",
    "```\n",
    "Input: x = [2.0, 3.0]\n",
    "\n",
    "    [2.0, 3.0]         ← 2 inputs\n",
    "         │\n",
    "         ▼\n",
    "    ┌─────────┐\n",
    "    │ Layer 0 │        4 neurons × 2 weights each\n",
    "    └─────────┘\n",
    "         │\n",
    "    [v, v, v, v]       ← 4 values\n",
    "         │\n",
    "         ▼\n",
    "    ┌─────────┐\n",
    "    │ Layer 1 │        4 neurons × 4 weights each\n",
    "    └─────────┘\n",
    "         │\n",
    "    [v, v, v, v]       ← 4 values\n",
    "         │\n",
    "         ▼\n",
    "    ┌─────────┐\n",
    "    │ Layer 2 │        1 neuron × 4 weights\n",
    "    └─────────┘\n",
    "         │\n",
    "       [v]             ← 1 final output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Component | Input | Output | Purpose |\n",
    "|-----------|-------|--------|---------|\n",
    "| **Neuron** | n values | 1 value | Weighted sum + activation |\n",
    "| **Layer** | n values | m values | Multiple neurons in parallel |\n",
    "| **MLP** | n values | k values | Layers connected in sequence |\n",
    "\n",
    "The beauty of using `Value` objects is that every operation builds a **computation graph**. When you call `.backward()` on the final output, gradients automatically propagate back through the entire network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
